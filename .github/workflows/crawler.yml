name: GitHub Crawler

# Trigger manually or on a schedule (every day at midnight)
on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  crawl:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: github
        options: >-
          --health-cmd="pg_isready -U postgres"
          --health-interval=5s
          --health-timeout=5s
          --health-retries=5
        ports:
          - 5432:5432  # Map container port to host

    env:
      # Use localhost because this job runs directly on the runner
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github
      GITHUB_ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Wait for Postgres to be ready
        run: |
          echo "Waiting for Postgres..."
          until pg_isready -h localhost -U postgres -d github; do
            sleep 2
          done

      - name: Setup DB schema
        run: python -m scripts.setup_db

      - name: Fetch 100 repositories via Python
        run: |
          python - << 'EOF'
          import os
          import requests
      
          token = os.environ['GITHUB_ACCESS_TOKEN']
          url = "https://api.github.com/graphql"
      
          query = """
          query($query: String!, $first: Int!, $after: String) {
            search(query: $query, type: REPOSITORY, first: $first, after: $after) {
              repositoryCount
              pageInfo {
                endCursor
                hasNextPage
              }
              nodes {
                __typename
                ... on Repository {
                  databaseId
                  name
                  owner { login }
                  stargazerCount
                  url
                }
              }
            }
            rateLimit { remaining resetAt cost }
          }
          """
      
          variables = {
              "query": "stars:>0",
              "first": 100,
              "after": None
          }
      
          resp = requests.post(url, json={"query": query, "variables": variables}, headers={"Authorization": f"bearer {token}"})
          data = resp.json()
          print(data)
          EOF

      - name: Crawl stars
        run: python -m src.app.main

      - name: Export database
        run: python -m scripts.export_data stars_history

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: repo-stars
          path: output/*.csv
